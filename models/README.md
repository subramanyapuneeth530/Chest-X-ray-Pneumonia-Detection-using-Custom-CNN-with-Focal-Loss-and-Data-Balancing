# CNN MODELS
## 1.First Model
### Model Overview
This document describes a custom-built Convolutional Neural Network (CNN) designed to classify chest X-ray images as either "NORMAL" or "PNEUMONIA." The model utilizes the Chest X-Ray Images (Pneumonia) dataset, with images preprocessed by resizing to 150x150 pixels and normalizing pixel values. The architecture consists of two convolutional blocks, each containing a Conv2D layer (with 32 and 64 filters respectively), MaxPooling2D for dimensionality reduction, and BatchNormalization to stabilize training. Following these blocks, a Flatten layer converts the feature maps into a 1D vector. To combat overfitting, a Dropout layer with a 50% rate is included. The network then feeds into a Dense layer with 128 neurons, culminating in a single-neuron output layer with a sigmoid activation function for binary classification. The model was trained for 10 epochs using the Adam optimizer and binary cross-entropy loss. Custom callbacks were used to monitor precision, recall, and F1-score during validation, providing a comprehensive view of performance beyond just accuracy.

### Results and Conclusion
The model demonstrated strong performance on the test set, achieving an accuracy of 82.8%. Crucially for medical applications, it exhibited a high recall of 87.7%, indicating its effectiveness in identifying true pneumonia cases and minimizing false negatives. The precision of 85.3% suggests that when the model predicts pneumonia, it is largely correct, reducing unnecessary follow-ups. The F1-score of 86.5% confirms a good balance between precision and recall.
Despite these promising results, particularly its high recall, the model has limitations. An overall accuracy below 90% suggests room for improvement in distinguishing between the two classes, especially in complex scenarios. The use of grayscale images and a relatively small input size (150x150) might lead to the omission of subtle diagnostic patterns. Furthermore, training for only 10 epochs may not have allowed for full convergence, potentially limiting its overall performance. In conclusion, while this lightweight CNN performs admirably and provides a high recall, which is highly beneficial in a medical diagnostic context ‚öïÔ∏è, further optimization through increased training epochs, higher-resolution imagery, and more sophisticated architectures could enhance its precision and overall robustness.

## 2nd model
### Model Overview
This model is a custom **Convolutional Neural Network (CNN)** designed for binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA" categories. It processes grayscale images resized to **224x224 pixels**, with pixel values normalized between 0 and 1 using **ImageDataGenerator**. The model's architecture is built using **TensorFlow Keras's Sequential API** and comprises three convolutional blocks. Each block consists of a **Conv2D layer** (with 32, 64, and 128 filters respectively, using 'relu' activation and 'same' padding), followed by a **MaxPooling2D layer** to reduce spatial dimensions, and a **BatchNormalization layer** to stabilize and accelerate training.
After feature extraction, a **Flatten layer** converts the 2D feature maps into a 1D vector. To prevent overfitting, two **Dropout layers** are included: one with a 0.5 rate after flattening and another with a 0.3 rate before the final output. The network then feeds into a **Dense layer** with 128 neurons (using 'relu' activation), leading to the final **Dense layer** with 1 neuron and a 'sigmoid' activation function for binary classification. The model is compiled using the **Adam optimizer** and **binary cross-entropy loss**, and trained for 10 epochs. Performance is evaluated using accuracy, precision, recall, and F1-score, along with confusion matrices for both training and test sets.

### Results and Conclusion
The model demonstrates strong performance on the **test set**, achieving an **accuracy of 83.81%**. Its **recall of 94.10%** is particularly noteworthy, indicating the model's exceptional ability to correctly identify positive pneumonia cases, which is crucial in a medical diagnostic context ‚öïÔ∏è. The **precision of 82.47%** shows that when the model predicts pneumonia, it is often correct, helping to minimize false positives. The **F1-score of 87.90%** further confirms a robust balance between precision and recall on unseen data.
However, a significant discrepancy exists between the training and test results. The **training accuracy is a low 49.25%**, with similarly low precision (49.23%), recall (47.58%), and F1-score (48.39%). This indicates that the model struggled to learn effectively from the training data, likely due to an **unbalanced training dataset** or issues with data shuffling that may not have been fully addressed, causing the model to underfit to the training distribution. The discrepancy between poor training metrics and strong test metrics suggests that the model might be generalizing well on certain aspects (perhaps simple cases of pneumonia), but its internal learning process during training was not optimal. This could be due to a class imbalance within the *training data* itself (even if the dataset is described as "balanced_train", further investigation is needed given the results), or the model's capacity might be too high for the complexity it captures during training, leading to it essentially "guessing" on the training set while somehow performing better on the test set.
In conclusion, while the model achieves impressive performance on the test set, especially in its ability to detect pneumonia (high recall), the extremely poor training metrics warrant immediate attention. This indicates a fundamental issue during the training phase that needs to be addressed. Future work should focus on thoroughly analyzing the training data's distribution and preprocessing, potentially increasing training epochs, or re-evaluating the model's complexity to ensure it effectively learns from the training data before achieving strong generalization.

## 3rd model
### Model Overview
The architecture comprises four convolutional blocks. Each block includes a **Conv2D layer** (with 32, 64, 128, and 256 filters, respectively, using 'relu' activation and 'same' padding), followed by **BatchNormalization** to stabilize training, and a **MaxPooling2D layer** for downsampling. After feature extraction, a **Flatten layer** transforms the 2D feature maps into a 1D vector. To mitigate overfitting, two **Dropout layers** are incorporated: one with a 0.5 rate after flattening and another with a 0.3 rate before the final output layer. The network concludes with a **Dense layer** of 128 neurons (using 'relu' activation) and a final **Dense layer** with a single neuron and 'sigmoid' activation for binary classification. The model is compiled with the **Adam optimizer** (with a learning rate of $1e-4$) and **binary cross-entropy loss**. Training is conducted for 10 epochs, with validation metrics (precision, recall, F1-score) calculated and displayed at the end of each epoch.

### Results and Conclusion
The model's performance on the **test set** shows a **moderate accuracy of 66.35%**. A striking result is the **perfect recall of 1.0000 (100%)**, indicating that the model successfully identified all actual pneumonia cases in the test set. While this is highly desirable in a medical context for minimizing false negatives ü©∫, it comes at the cost of a **lower precision of 0.6500**. This suggests a high rate of **false positives**, meaning the model frequently misclassifies normal cases as pneumonia. The **F1-score of 0.7879** reflects this imbalance, showing a compromise between recall and precision.
The training process also revealed some insights. The model was trained for 10 epochs, and the training and validation accuracy over epochs were plotted. While not explicitly provided in the summary, the high recall and low precision on the test set point to a potential **bias towards predicting the positive class (pneumonia)**. This could stem from the training data's characteristics, the loss function, or the model's architecture encouraging this behavior. Although achieving 100% recall is impressive for critical case detection, the trade-off of numerous false positives could lead to unnecessary further examinations or patient anxiety.
In conclusion, while the model excels at identifying every instance of pneumonia (perfect recall), its relatively low precision and overall accuracy suggest that it struggles to differentiate accurately between normal and pneumonia cases, leading to a significant number of false alarms. Future improvements should focus on **balancing precision and recall**, perhaps by adjusting class weights, employing different loss functions, or implementing more sophisticated data augmentation techniques to help the model learn a more robust decision boundary and reduce false positives without severely impacting its high recall.

## 4th model
### Model Overview
This CNN model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images resized to **256x256 pixels**, with pixel values normalized between 0 and 1. The model is configured to handle **3-channel (color) images**, although X-rays are typically grayscale.
The architecture features four convolutional blocks, each consisting of a **Conv2D layer** (with 32, 64, 128, and 256 filters, using 'relu' activation and 'same' padding), followed by **BatchNormalization** and **MaxPooling2D**. Following these blocks, a **Flatten layer** transforms the data into a 1D vector. To prevent overfitting, two **Dropout layers** are included: one with a 0.5 rate and another with a 0.3 rate. The network then feeds into a **Dense layer** of 128 neurons (with 'relu' activation), culminating in a single-neuron output layer with a 'sigmoid' activation for binary classification. The model is compiled with the **Adam optimizer** (with a learning rate of $1e-4$) and **binary cross-entropy loss**.
Crucially, the training process incorporates **callbacks** for improved robustness: **EarlyStopping** monitors validation loss with a patience of 5 epochs and restores the best weights, preventing overfitting and unnecessary training. **ReduceLROnPlateau** reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 3 consecutive epochs, aiding convergence. The model is set to train for 30 epochs, though EarlyStopping may halt it sooner. Performance is tracked by monitoring accuracy, precision, recall, and F1-score on the validation set at the end of each epoch, and comprehensive evaluation is performed on the test set.

### Results and Conclusion
The improved model demonstrates **significantly better performance** on the **test set** compared to previous iterations, achieving an **accuracy of 86.38%**. It maintains a high **recall of 0.9333 (93.33%)**, indicating its continued effectiveness in identifying true pneumonia cases while also improving its **precision to 0.8605**. This higher precision suggests a reduction in false positives compared to the prior model, leading to more reliable predictions. The **F1-score of 0.8954** further confirms a strong balance between precision and recall, signifying a robust overall performance.
The inclusion of **EarlyStopping** and **ReduceLROnPlateau** callbacks played a vital role in enhancing the model's stability and generalization. EarlyStopping prevented overfitting by halting training when the validation loss ceased to improve, ensuring the model retains its best-performing weights. ReduceLROnPlateau helped the optimizer navigate the loss landscape more effectively, preventing the learning process from getting stuck in local minima. These callbacks likely contributed to the improved training dynamics and the more balanced precision-recall trade-off observed in the final results.
In conclusion, this refined CNN model represents a substantial improvement in classifying chest X-ray images for pneumonia detection ü©∫. The higher accuracy, combined with strong precision and recall, makes it a more reliable tool for medical diagnosis. The strategic use of callbacks effectively managed the training process, leading to a more robust and generalized model. Future work could explore more advanced architectures, transfer learning techniques, or specialized data augmentation for further performance gains, particularly in situations with highly imbalanced datasets or subtle visual patterns.

## 5th model
### Model Overview
This CNN model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images that are now resized to a larger **320x320 pixels**, with pixel values normalized between 0 and 1. The model is configured to handle **3-channel (color) images**, although chest X-rays are typically grayscale.
The architecture remains consistent with the previous iteration, featuring four convolutional blocks. Each block includes a **Conv2D layer** (with 32, 64, 128, and 256 filters, using 'relu' activation and 'same' padding), followed by **BatchNormalization** and **MaxPooling2D**. A **Flatten layer** then transforms the data into a 1D vector. To prevent overfitting, two **Dropout layers** are included: one with a 0.5 rate and another with a 0.3 rate. The network concludes with a **Dense layer** of 128 neurons (with 'relu' activation) and a final **Dense layer** with a single neuron and 'sigmoid' activation for binary classification. The model is compiled with the **Adam optimizer** (with a learning rate of $1e-4$) and **binary cross-entropy loss**.
The training process is set for an increased **50 epochs**, but continues to leverage **EarlyStopping** (monitoring 'val_loss' with patience 5) and **ReduceLROnPlateau** (monitoring 'val_loss' with patience 3, factor 0.5) callbacks. These callbacks are crucial for preventing overfitting and optimizing the learning rate during training. Performance is continuously monitored by tracking accuracy, precision, recall, and F1-score on the validation set after each epoch, with comprehensive evaluation performed on the test set at the end.

### Results and Conclusion
The current model's performance on the **test set** shows an **accuracy of 77.08%**. While this is a decrease from the previous model, it maintains an exceptionally high **recall of 0.9846 (98.46%)**, indicating its strong capability to identify nearly all actual pneumonia cases. However, the **precision stands at 0.7370**, which is also lower than the previous iteration. This suggests an increased rate of **false positives**; the model is highly sensitive to pneumonia but at the cost of frequently misclassifying normal cases as pneumonia. The **F1-score of 0.8430** reflects this trade-off, indicating a slightly less balanced performance compared to the previous model.
The increase in input image size to 320x320 pixels and the extended number of epochs (up to 50) did not yield a direct improvement in overall accuracy or precision, as seen in the test metrics. Instead, the model exhibits an even stronger bias towards maximizing recall, potentially at the expense of precision. This behavior, while valuable in scenarios where missing a positive case (pneumonia) is critical, could lead to a higher burden of unnecessary follow-up diagnostics. The callbacks (EarlyStopping and ReduceLROnPlateau) are still active, but they haven't entirely mitigated the strong bias towards recall that results in lower precision.
In conclusion, this iteration of the CNN model excels at **detecting almost all pneumonia cases**, achieving near-perfect recall. However, the trade-off is a notable drop in precision, meaning a higher number of false alarms. For critical medical diagnoses ü©∫, prioritizing recall is often desired, but the balance between false positives and false negatives needs careful consideration. Future efforts should aim to **rebalance precision and recall**, possibly by adjusting the classification threshold, exploring different loss functions (e.g., focal loss for imbalanced learning), or applying more targeted data augmentation strategies that specifically help distinguish between normal and early-stage pneumonia cases, especially with the increased image resolution.

## 6th model
### Model Overview
This **Convolutional Neural Network (CNN)** model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images resized to **256x256 pixels** and normalizes their pixel values between 0 and 1. A key enhancement in this iteration is the implementation of **data augmentation** during training, including `rotation_range`, `zoom_range`, and `horizontal_flip`, to increase the diversity of the training data and improve the model's generalization capabilities. The model is configured to handle **3-channel (color) images**, although X-rays are inherently grayscale, which is a common practice when working with pre-trained models or frameworks expecting color inputs.
The model's architecture comprises three convolutional blocks. Each block features a **Conv2D layer** (with 32, 64, and 128 filters respectively, using 'relu' activation), followed by **MaxPooling2D** for dimensionality reduction, and **BatchNormalization** to stabilize and speed up training. After feature extraction, a **Flatten layer** converts the 2D feature maps into a 1D vector. This is followed by a **Dense layer** with 256 neurons (using 'relu' activation) and a **Dropout layer** with a 0.4 rate to prevent overfitting. The network concludes with a single-neuron output layer with a **sigmoid activation function** for binary classification. The model is compiled using the **Adam optimizer** with a **learning rate of $1e-4$** and **binary cross-entropy loss**.
The training process is set for 20 epochs and incorporates two critical callbacks: **ReduceLROnPlateau** and **EarlyStopping**. **ReduceLROnPlateau** reduces the learning rate by half if the validation loss doesn't improve for 2 consecutive epochs, aiding in fine-tuning. **EarlyStopping** halts training if the validation loss doesn't improve for 4 consecutive epochs, restoring the best weights learned during training, effectively preventing overfitting and optimizing training time.

### Results and Conclusion
The model demonstrates **excellent performance** on the **test set**, achieving an **accuracy of 91.19%**. This marks a significant improvement over previous iterations. The **precision of 0.91** indicates that when the model predicts pneumonia, it is correct 91% of the time, effectively minimizing false positives. The **recall of 0.96** is also very high, meaning the model successfully identifies 96% of all actual pneumonia cases, which is crucial for a medical diagnostic tool ü©∫. The **F1-score of 0.93** highlights a strong balance between precision and recall, signifying robust overall performance.
The **implementation of data augmentation** and the refined use of **callbacks (ReduceLROnPlateau and EarlyStopping)** are likely the primary contributors to this notable improvement. Data augmentation effectively diversified the training set, making the model more robust and less prone to overfitting on specific image variations. The callbacks efficiently guided the training process, preventing the model from becoming overfitted and ensuring optimal learning. The convergence of training and validation accuracy and loss over epochs, as shown in the plots, further supports the stability and effectiveness of the training.
In conclusion, this CNN model represents a highly effective solution for chest X-ray pneumonia classification. The strategic combination of a robust architecture, comprehensive data augmentation, and intelligent training callbacks has resulted in a model that achieves high accuracy, precision, and recall on unseen data. This model offers a promising foundation for a reliable diagnostic aid in clinical settings, providing a good balance between identifying true pneumonia cases and minimizing misclassifications.

## 7th model
### Model Overview
This **Convolutional Neural Network (CNN)** model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA" categories. It processes images resized to **256x256 pixels** and normalizes their pixel values. To enhance generalization, **data augmentation** techniques like rotation, zoom, and horizontal flipping are applied during training. The model is configured to handle **3-channel (color) images**, a common practice for CNNs.
The core of the model consists of **three reusable convolutional blocks**, each defined by a `conv_block` function. Each block comprises two **Conv2D layers** (using 'relu' activation and 'same' padding), followed by **BatchNormalization** to stabilize training, **MaxPooling2D** for downsampling, and a **Dropout layer** with a 0.3 rate to mitigate overfitting. The filters in these blocks progressively increase from 32 to 128. After these convolutional layers, a **GlobalAveragePooling2D** layer reduces each feature map to a single value, making the model more robust to spatial translations. This is followed by a **Dense layer** with 256 neurons (using 'relu' activation) and another **Dropout layer** with a 0.5 rate. The final output layer is a single neuron with a **sigmoid activation function** for binary classification. The model is compiled using the **Adam optimizer** with a learning rate of $1e-4$ and **binary cross-entropy loss**.
Training is set for 25 epochs and incorporates **EarlyStopping** (patience 4, restores best weights) and **ReduceLROnPlateau** (patience 2, factor 0.5) callbacks. These callbacks are crucial for preventing overfitting and dynamically adjusting the learning rate to optimize the training process.

### Results and Conclusion
The model achieves a **test accuracy of 81.09%**. It demonstrates a **precision of 0.84**, meaning 84% of its pneumonia predictions are correct, and a **recall of 0.86**, indicating it identifies 86% of actual pneumonia cases. The **F1-score of 0.85** reflects a good balance between precision and recall.
Compared to previous iterations, this model shows a slight decrease in overall accuracy. However, the use of **GlobalAveragePooling2D** in place of a Flatten layer in combination with the repeated `conv_block` structure aims to improve the model's ability to learn more robust and spatially invariant features. The increased dropout rates (0.3 in `conv_block` and 0.5 after the dense layer) are designed to further combat overfitting, which is often a challenge with medical image datasets.
In conclusion, this CNN model, with its refined architecture using reusable convolutional blocks and GlobalAveragePooling2D, along with robust data augmentation and callbacks, provides a solid performance for chest X-ray pneumonia classification ü©∫. While the accuracy is slightly lower than the best previous result, the balanced precision and recall, along with a more streamlined and potentially more robust architecture, make it a valuable model. Further fine-tuning, possibly through hyperparameter optimization or exploring different base architectures, could lead to even better performance.

## 8th model
### Model Overview
This **Convolutional Neural Network (CNN)** model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images at a **higher resolution of 320x320 pixels** and normalizes their pixel values. To improve generalization, **data augmentation** techniques, including rotation, zooming, and horizontal flipping, are applied during training. The model is configured to expect **3-channel (color) images**, even though X-rays are typically grayscale, a common practice for leveraging CNN architectures.
The model's architecture is significantly deeper than previous iterations, featuring **five convolutional blocks**. Each block consists of a **Conv2D layer** (with increasing filters: 32, 64, 128, 256, and 512, all using 'relu' activation), followed by **MaxPooling2D** for spatial downsampling, and **BatchNormalization** to stabilize training. After these extensive feature extraction layers, the data is **Flattened** into a 1D vector. This is followed by two **Dense layers** with 256 and 128 neurons respectively (both with 'relu' activation), each paired with a **Dropout layer** (0.4 and 0.3 rates) to prevent overfitting. The network culminates in a single-neuron output layer with a **sigmoid activation function** for binary classification. The model is compiled using the **Adam optimizer** with a **learning rate of $1e-4$** and **binary cross-entropy loss**.
The training process is set for an extended **50 epochs**. Notably, the **EarlyStopping callback has been removed**, allowing the model to train for the full duration unless the learning rate reduction mechanism intervenes. Only the **ReduceLROnPlateau callback** is used, reducing the learning rate by half if the validation loss doesn't improve for 2 consecutive epochs. The total training time is also measured and reported.

### Results and Conclusion
The model demonstrates **excellent performance** on the **test set**, achieving an **accuracy of 90.87%**. It shows a strong balance between **precision (0.93)** and **recall (0.92)**, indicating that it is highly accurate in its pneumonia predictions while also effectively identifying most actual pneumonia cases. The **F1-score of 0.93** further solidifies its robust overall performance.
The increase in image resolution to 320x320 pixels and the deeper architecture with five convolutional blocks (including higher filter counts) seem to have contributed positively to the model's ability to learn more intricate features. The sustained training for 50 epochs, without early stopping, combined with the adaptive learning rate via **ReduceLROnPlateau**, allowed the model ample opportunity to converge and refine its weights. While the training time is longer due to the increased complexity and epochs, the resulting performance metrics indicate that these architectural and training adjustments were beneficial.
In conclusion, this deeper CNN model, leveraging higher-resolution input images, extensive convolutional layers, and sustained training with dynamic learning rate adjustment, achieves impressive and well-balanced performance in classifying chest X-ray images for pneumonia ü©∫. The high accuracy, precision, and recall make it a strong candidate for a reliable diagnostic aid. Future considerations could involve exploring more advanced regularization techniques or incorporating transfer learning from pre-trained models for even further enhancements.

## 9th model
### Model Overview
This **Convolutional Neural Network (CNN)** model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images at a resolution of **320x320 pixels** and normalizes their pixel values. To improve generalization, **data augmentation** techniques, including rotation, zooming, and horizontal flipping, are applied during training. The model is configured to handle **3-channel (color) images**.
The model's architecture consists of **four convolutional blocks**. Each block features a **Conv2D layer** (with increasing filters: 32, 64, 128, and 256, all using 'relu' activation), followed by **MaxPooling2D** for spatial downsampling, and **BatchNormalization** to stabilize training. After feature extraction, the data is **Flattened** into a 1D vector. This is followed by a **Dense layer** with 256 neurons (with 'relu' activation) and a **Dropout layer** with a 0.5 rate to prevent overfitting. The network culminates in a single-neuron output layer with a **sigmoid activation function** for binary classification.
A key enhancement in this iteration is the use of **Focal Loss** as the loss function, implemented as a custom function with `gamma=2.0` and `alpha=0.25`. Focal Loss is particularly effective in addressing **class imbalance** by down-weighting the loss contribution from well-classified examples, thus focusing training on hard-to-classify examples. Additionally, **class weights** are computed and applied during training using `sklearn.utils.class_weight.compute_class_weight` to further counteract any potential class imbalance in the training data. The model is compiled using the **Adam optimizer** with a **learning rate of $1e-4$** and this custom `focal_loss`. Training is set for 50 epochs.

### Results and Conclusion
The model's performance on the **test set** shows an **accuracy of 78.85%**. It achieves an exceptionally high **recall of 0.99 (99%)**, indicating that the model is extremely effective at identifying actual pneumonia cases, minimizing false negatives. However, the **precision is 0.75**, which means that 75% of its pneumonia predictions are correct, implying a notable number of false positives. The **F1-score of 0.85** reflects the trade-off between the very high recall and the moderate precision.
The integration of **Focal Loss** and **class weighting** was aimed at mitigating class imbalance and directing the model's focus to more challenging examples. While this approach has resulted in an almost perfect recall, it has also led to a decrease in overall accuracy and precision compared to the previous model (which had 90.87% accuracy and 0.93 precision). This indicates that the model is now even more **biased towards predicting the positive class (pneumonia)** to ensure no actual pneumonia cases are missed. This behavior is often desirable in medical diagnosis where false negatives can have severe consequences, but it comes at the cost of increased false positives.
In conclusion, this CNN model with **Focal Loss and class weighting** excels at **detecting nearly all pneumonia cases** (exceptionally high recall), making it highly sensitive to the disease ü©∫. However, this sensitivity leads to a higher rate of false positives, which could result in unnecessary further investigations. For clinical deployment, the acceptable balance between false positives and false negatives needs to be carefully considered. Future work might involve fine-tuning the `gamma` and `alpha` parameters of Focal Loss, experimenting with different thresholds for prediction, or exploring alternative strategies to achieve a better balance between high recall and improved precision.

## table of results
```
| Metric        | Model 1    | Model 2 | Model 3 | Model 4 | Model 5 | Model 6 | Model 7 | Model 8 | Model 9 |
|---------------|------------|---------|---------|---------|---------|---------|---------|---------|---------|
| Test Accuracy | 82.8%      | 83.81%  | 66.35%  | 86.38%  | 77.08%  | 91.19%  | 81.09%  | 90.87%  | 78.85%  |
| Precision     | 85.3%      | 82.47%  | 65.00%  | 86.05%  | 73.70%  | 91.0%   | 84.0%   | 93.0%   | 75.0%   |
| Recall        | 87.7%      | 94.10%  | 100.0%  | 93.33%  | 98.46%  | 96.0%   | 86.0%   | 92.0%   | 99.0%   |
| F1 Score      | 86.5%      | 87.90%  | 78.79%  | 89.54%  | 84.30%  | 93.0%   | 85.0%   | 93.0%   | 85.0%   |
```
## Model 6: The Optimal Performer
The model identified as "Model 6" (the one with 91.19% accuracy, 0.91 precision, 0.96 recall, and 0.93 F1-score) stands out as the best performer among all the iterations. Its superior performance is primarily due to its exceptional balance of high precision and recall, combined with the highest overall accuracy. While a high recall is critical in medical diagnostics to minimize missed cases, achieving it without significantly sacrificing precision (which reduces false alarms) is the hallmark of a robust and clinically useful model ü©∫. This balance was likely attained through effective data augmentation, a well-suited model architecture, and the strategic use of EarlyStopping and ReduceLROnPlateau callbacks, which collectively optimized the learning process and prevented overfitting.
