# CNN MODELS
## 1.First Model
### Model Overview
This document describes a custom-built Convolutional Neural Network (CNN) designed to classify chest X-ray images as either "NORMAL" or "PNEUMONIA." The model utilizes the Chest X-Ray Images (Pneumonia) dataset, with images preprocessed by resizing to 150x150 pixels and normalizing pixel values. The architecture consists of two convolutional blocks, each containing a Conv2D layer (with 32 and 64 filters respectively), MaxPooling2D for dimensionality reduction, and BatchNormalization to stabilize training. Following these blocks, a Flatten layer converts the feature maps into a 1D vector. To combat overfitting, a Dropout layer with a 50% rate is included. The network then feeds into a Dense layer with 128 neurons, culminating in a single-neuron output layer with a sigmoid activation function for binary classification. The model was trained for 10 epochs using the Adam optimizer and binary cross-entropy loss. Custom callbacks were used to monitor precision, recall, and F1-score during validation, providing a comprehensive view of performance beyond just accuracy.

### Results and Conclusion
The model demonstrated strong performance on the test set, achieving an accuracy of 82.8%. Crucially for medical applications, it exhibited a high recall of 87.7%, indicating its effectiveness in identifying true pneumonia cases and minimizing false negatives. The precision of 85.3% suggests that when the model predicts pneumonia, it is largely correct, reducing unnecessary follow-ups. The F1-score of 86.5% confirms a good balance between precision and recall.
Despite these promising results, particularly its high recall, the model has limitations. An overall accuracy below 90% suggests room for improvement in distinguishing between the two classes, especially in complex scenarios. The use of grayscale images and a relatively small input size (150x150) might lead to the omission of subtle diagnostic patterns. Furthermore, training for only 10 epochs may not have allowed for full convergence, potentially limiting its overall performance. In conclusion, while this lightweight CNN performs admirably and provides a high recall, which is highly beneficial in a medical diagnostic context ‚öïÔ∏è, further optimization through increased training epochs, higher-resolution imagery, and more sophisticated architectures could enhance its precision and overall robustness.

## 2nd model
### Model Overview
This model is a custom **Convolutional Neural Network (CNN)** designed for binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA" categories. It processes grayscale images resized to **224x224 pixels**, with pixel values normalized between 0 and 1 using **ImageDataGenerator**. The model's architecture is built using **TensorFlow Keras's Sequential API** and comprises three convolutional blocks. Each block consists of a **Conv2D layer** (with 32, 64, and 128 filters respectively, using 'relu' activation and 'same' padding), followed by a **MaxPooling2D layer** to reduce spatial dimensions, and a **BatchNormalization layer** to stabilize and accelerate training.
After feature extraction, a **Flatten layer** converts the 2D feature maps into a 1D vector. To prevent overfitting, two **Dropout layers** are included: one with a 0.5 rate after flattening and another with a 0.3 rate before the final output. The network then feeds into a **Dense layer** with 128 neurons (using 'relu' activation), leading to the final **Dense layer** with 1 neuron and a 'sigmoid' activation function for binary classification. The model is compiled using the **Adam optimizer** and **binary cross-entropy loss**, and trained for 10 epochs. Performance is evaluated using accuracy, precision, recall, and F1-score, along with confusion matrices for both training and test sets.

### Results and Conclusion
The model demonstrates strong performance on the **test set**, achieving an **accuracy of 83.81%**. Its **recall of 94.10%** is particularly noteworthy, indicating the model's exceptional ability to correctly identify positive pneumonia cases, which is crucial in a medical diagnostic context ‚öïÔ∏è. The **precision of 82.47%** shows that when the model predicts pneumonia, it is often correct, helping to minimize false positives. The **F1-score of 87.90%** further confirms a robust balance between precision and recall on unseen data.
However, a significant discrepancy exists between the training and test results. The **training accuracy is a low 49.25%**, with similarly low precision (49.23%), recall (47.58%), and F1-score (48.39%). This indicates that the model struggled to learn effectively from the training data, likely due to an **unbalanced training dataset** or issues with data shuffling that may not have been fully addressed, causing the model to underfit to the training distribution. The discrepancy between poor training metrics and strong test metrics suggests that the model might be generalizing well on certain aspects (perhaps simple cases of pneumonia), but its internal learning process during training was not optimal. This could be due to a class imbalance within the *training data* itself (even if the dataset is described as "balanced_train", further investigation is needed given the results), or the model's capacity might be too high for the complexity it captures during training, leading to it essentially "guessing" on the training set while somehow performing better on the test set.
In conclusion, while the model achieves impressive performance on the test set, especially in its ability to detect pneumonia (high recall), the extremely poor training metrics warrant immediate attention. This indicates a fundamental issue during the training phase that needs to be addressed. Future work should focus on thoroughly analyzing the training data's distribution and preprocessing, potentially increasing training epochs, or re-evaluating the model's complexity to ensure it effectively learns from the training data before achieving strong generalization.

## 3rd model
### Model Overview
The architecture comprises four convolutional blocks. Each block includes a **Conv2D layer** (with 32, 64, 128, and 256 filters, respectively, using 'relu' activation and 'same' padding), followed by **BatchNormalization** to stabilize training, and a **MaxPooling2D layer** for downsampling. After feature extraction, a **Flatten layer** transforms the 2D feature maps into a 1D vector. To mitigate overfitting, two **Dropout layers** are incorporated: one with a 0.5 rate after flattening and another with a 0.3 rate before the final output layer. The network concludes with a **Dense layer** of 128 neurons (using 'relu' activation) and a final **Dense layer** with a single neuron and 'sigmoid' activation for binary classification. The model is compiled with the **Adam optimizer** (with a learning rate of $1e-4$) and **binary cross-entropy loss**. Training is conducted for 10 epochs, with validation metrics (precision, recall, F1-score) calculated and displayed at the end of each epoch.

### Results and Conclusion
The model's performance on the **test set** shows a **moderate accuracy of 66.35%**. A striking result is the **perfect recall of 1.0000 (100%)**, indicating that the model successfully identified all actual pneumonia cases in the test set. While this is highly desirable in a medical context for minimizing false negatives ü©∫, it comes at the cost of a **lower precision of 0.6500**. This suggests a high rate of **false positives**, meaning the model frequently misclassifies normal cases as pneumonia. The **F1-score of 0.7879** reflects this imbalance, showing a compromise between recall and precision.
The training process also revealed some insights. The model was trained for 10 epochs, and the training and validation accuracy over epochs were plotted. While not explicitly provided in the summary, the high recall and low precision on the test set point to a potential **bias towards predicting the positive class (pneumonia)**. This could stem from the training data's characteristics, the loss function, or the model's architecture encouraging this behavior. Although achieving 100% recall is impressive for critical case detection, the trade-off of numerous false positives could lead to unnecessary further examinations or patient anxiety.
In conclusion, while the model excels at identifying every instance of pneumonia (perfect recall), its relatively low precision and overall accuracy suggest that it struggles to differentiate accurately between normal and pneumonia cases, leading to a significant number of false alarms. Future improvements should focus on **balancing precision and recall**, perhaps by adjusting class weights, employing different loss functions, or implementing more sophisticated data augmentation techniques to help the model learn a more robust decision boundary and reduce false positives without severely impacting its high recall.

## 4th model
### Model Overview
This CNN model is designed for the binary classification of chest X-ray images into "NORMAL" or "PNEUMONIA." It processes images resized to **256x256 pixels**, with pixel values normalized between 0 and 1. The model is configured to handle **3-channel (color) images**, although X-rays are typically grayscale.
The architecture features four convolutional blocks, each consisting of a **Conv2D layer** (with 32, 64, 128, and 256 filters, using 'relu' activation and 'same' padding), followed by **BatchNormalization** and **MaxPooling2D**. Following these blocks, a **Flatten layer** transforms the data into a 1D vector. To prevent overfitting, two **Dropout layers** are included: one with a 0.5 rate and another with a 0.3 rate. The network then feeds into a **Dense layer** of 128 neurons (with 'relu' activation), culminating in a single-neuron output layer with a 'sigmoid' activation for binary classification. The model is compiled with the **Adam optimizer** (with a learning rate of $1e-4$) and **binary cross-entropy loss**.
Crucially, the training process incorporates **callbacks** for improved robustness: **EarlyStopping** monitors validation loss with a patience of 5 epochs and restores the best weights, preventing overfitting and unnecessary training. **ReduceLROnPlateau** reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 3 consecutive epochs, aiding convergence. The model is set to train for 30 epochs, though EarlyStopping may halt it sooner. Performance is tracked by monitoring accuracy, precision, recall, and F1-score on the validation set at the end of each epoch, and comprehensive evaluation is performed on the test set.

### Results and Conclusion
The improved model demonstrates **significantly better performance** on the **test set** compared to previous iterations, achieving an **accuracy of 86.38%**. It maintains a high **recall of 0.9333 (93.33%)**, indicating its continued effectiveness in identifying true pneumonia cases while also improving its **precision to 0.8605**. This higher precision suggests a reduction in false positives compared to the prior model, leading to more reliable predictions. The **F1-score of 0.8954** further confirms a strong balance between precision and recall, signifying a robust overall performance.
The inclusion of **EarlyStopping** and **ReduceLROnPlateau** callbacks played a vital role in enhancing the model's stability and generalization. EarlyStopping prevented overfitting by halting training when the validation loss ceased to improve, ensuring the model retains its best-performing weights. ReduceLROnPlateau helped the optimizer navigate the loss landscape more effectively, preventing the learning process from getting stuck in local minima. These callbacks likely contributed to the improved training dynamics and the more balanced precision-recall trade-off observed in the final results.
In conclusion, this refined CNN model represents a substantial improvement in classifying chest X-ray images for pneumonia detection ü©∫. The higher accuracy, combined with strong precision and recall, makes it a more reliable tool for medical diagnosis. The strategic use of callbacks effectively managed the training process, leading to a more robust and generalized model. Future work could explore more advanced architectures, transfer learning techniques, or specialized data augmentation for further performance gains, particularly in situations with highly imbalanced datasets or subtle visual patterns.
